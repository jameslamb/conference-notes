# apply() conf 2021

Dates: April 21-22, 2021
Link: https://www.applyconf.com/

This conference came out of "the MLOps community meetup".

## Day 1

### A. Rethinking Feature Stores with Feast and Tecton

* when you're iterating on new features, it's important to be able to automatically backfill feature values
* "the feature store is the interface between your models and your application"
    - feature stores can also do some data quality monitoring / drift detection
    - e.g. they might automate an analysis of how the distribution of some features changes over time
* feature stores re-use your existing infra in your cloud environment
    - not talking about a new type of database / data warehouse
* Tecton is contributing to Feast, as a possible "reference architecture" for feature stores

### B. Apache Arrow and the Next Generation of Data Systems

* speaker: Wes McKinney
* some issues:
    - many workloads dominated by serialization overhead
    - inconsistent data access / data delivery performance
    - data "held hostage" by a vertically-integration system
    - many systems operating on stale data awaiting ETL jobs
* compute engine silos
    - highly variable and inconsistent performance (for example, consider all the different data frame libraries available)
    - hardware is frequently underutilized
    - hardware innovations are slow to be utilized
    - "engine lock-in" may add constraints in the "data" and "language" layers (e.g. `{dplyr}`)
* language silos
    - lack of code sharing
    - using a preferred language can cause massive performance penalties
* the goal
    - extending systems with user-defined code should incur 0 overhead due to serialization
* Julia is the newest language joining Apache Arrow
* all of these proejcts are using Arrow in some way
    - Spark
    - BigQuery
    - Parquet
    - Dremio
    - snowflake
    - Azure
    - R
    - pandas
    - DuckDB
    - Amazon Athena
    - Tensorflow Extended
* "Arrow-native computing"
    - NVIDIA RAPIDS
    - BlazingSQL
    - dremio
    - DataFusion
    - Cylon
    - NoisePage
    - vaex.io
* new projects
    - https://github.com/ritchie46/polars (Rust-based dataframe project, based on Arrow DataFusion)
* it's pronounced "par-KAY"
* "Google warehouses a lot of data in protobuf"
    - one thing that sucks about that is that you have to get the `.proto` file and run a code-generator to access the data
    - there are definitely use cases for protobuf that don't make sense for Arrow. For example highly-sparse data where most fields are mostly-null would be better with protobuf

### C. Machine Learning is Going Real-Time

* Chip Huyen, ML professor @ Stanford University
    - did some ML stuff at Netflix, Snorkle, NVIDIA
* what is real-time?
    - philosopher: "there's no such thing as time"
    - software engineer: "there's no such thing as real-time"
* types of real-time in ML
    - online training (on the order of minutes)
    - online scoring (on the order of seconds)
* she showed some examples of how increasing latency has business impact
    - Google: quadrupling latency causes a 0.5% drop in searches
    - "if you take too long, users will click on something else"
* batch predictions
    - generate all the predictions offline before users do something, and then you store them and look them up
    - for example, this is how Netflix works. The recommender is precomputed
    - can't adapt to changing interests
    - you have to have a finite input space (know exactly how many predictions to generate)
* make models smaller
    - quantization (store lower-precision data, e.g. integer inference)
    - knowledge distillation
    - pruning
    - low-rank factorization
* streaming storage
    - Kafka, Kinesis, Flink, Samza
* a REST API is "request-driven"
    - there is a client that asks for things from a server
* streaming becomes more attractive when "inter-service communication" cost becomes impactful
* stream processing is a fundamentally different thing, and requires a mental shift
* why you might care about online learning
    - handling rare events
    - on-device training after a model has been deployed
* "train your model with a sufficient number of epochs until convergence and evaluate on a static test set"
    - no epoch (each data point seen only once)
    - no static test set

### D. Third Gen, Production Ready ML architectures

* Speaker: Waleed Kadous, Anyscale
* generations of ML pipelines
    - 1st gen: "fixed function" pipeline
    - 2nd gen: programmability within the pipeline
    - 3rd gen: "complete programmability"
* "Are Ray and its siblings the next generation of ML architectures?"
* ML systems "require distributed training and distributed inference"
* programmability in GPUs (history)
    - C-like language (GLSL, HLSL)
    - still had to basically conform to the existing pipeline
    - scene in, pixels out
* third gen of GPUs
    - everything is totally programmable
    - interface is a programming language
    - led to an explosion of applications
        - e.g. `cuDNN --> Caffe --> Torch --> PyTorch`
    - now people write games with an engine like Unity or Unreal Engine 4
    - focus is on libraries, the GPU is just a detail now
    - nobody talkes about OpenGL now
    - originally, when you talked to people building games they didn't think about libraries, they talked about "making things run on OpenGL". But with these libraries for programming GPUs, it now just made it "just like using any other library"
* just like what happened with GPU computing, maybe ML will shift to a focus on libraries
    - where "the compute engine is just a detail"
    - opens up machine learning to a huge number of users
    - you don't need to think about a specific type of EC2 instance (for example), you just use a library
* Ray
    - a cloud-provider independent compute launcher / autoscaler
    - an ecosystem of distributed computaiton libraries
    - there's no huge cost to creating tasks, and tasks can create other tasks
* Uber is using Ray in Michelangelo 3
    - Uber: "...Ray will continue to play an ... important role in ... common infrastructure and standardization to the production machine learning ecosystem"
* Ray Summit coming up in June
* "you can do Dask on top of Ray"
* Q: have you tried Ray for federated learning?
    - no not really, we don't have much of a presence on mobile devices (and neither does Python in general)
    - Ray does support Java and C++, by the way
* Q: how does Ray handle observability?
    - there's a dashboard

### E. Metaflow: Supercharging our data scientists's productivity

* speakers: Ravi Kiran Chirravuri, Jan Forjanczyk (Netflix)
* this guy focuses on the problem of "content demand modeling" (predicting how popular a show will be)
* metaflow started from two observations:
    - most data science projects at Netflix seemed similar
    - data scientists wanted flexibility at "high" levels of the stack (data prep, experimentation) but were ok giving control to an opinionated tool at "lower" levels (like what infrastructure you get provisioned onto)
* metaflow looks really similar to Ray, Dask delayed, and Prefect (at least in Python). You use a decorator `@step` to group things together into a DAG
    - you also use a decorator `@resources` like `@resources(cpu=16)`, `@resources(memory=200000)`
    - you also use a decorator `@conda(libs={'tensorflow': '1.14'})` to say "this step needs this library"

### F. Data Observability: The Next Frontier of Data Engineer

* Barr Moses, CEO of Monte Carlo
* pillars of data observability
    - freshness
    - distribution
    - volume
    - schema
    - lineage
* yotpo

### G. Hamilton: A Micro Framework for Creating Dataframes

* speaker: Stefan Krawczyk
* this is an internal project at Stitch Fix
* https://multithreaded.stitchfix.com/blog/2021/04/05/MITW/

### H. Developing Fraud Detection in Tecton

* speaker: Matt Bleifer, Tecton
* this was a sales talk mostly

### I. Redis as an online feature store

* speaker: Taimur Rashid, Chief Business Officer at Redis Labs
* customers use Redis as their "online feature store"
* people want something "live, fast, and fresh"

### J. Evolution and Unification of Pinterest ML Platform

* speaker: David Liu, Pinterest
* pinterest blog: https://medium.com/pinterest-engineering/machine-learning/home
* large-scale ML
    - recommender / ranking of pins
    - distributed tensorflow / pytorch
* important to separate `DataType` ("computer science data type") and `FeatureType` (the sort of "staitstical data type")
* Pinterest FeatureHub = a self-serve UI for the pinterest internal feature store
* Pinterest used `mlflow` for model registry
    - then they built their own deployment and "inference" services on top of MLFlow (called "Scorpion Model Server")

### K. MLOps done right with Centralized Model Performance Management powered by XAI

* speaker: Krishna Gade, Fiddler
* Fiddler - an "APM" for machine learning

### L. Feature Stores at Tide

* speaker: Hendrik Brackmann, Director of Data at Tide
* Tide is a financial platform for "really small businesses"
* problems solved by feature store
    - business metrics were a funciton of both rules and ML systems
    - data not accessible in real time
    - deployments often caused unexpected outages

### M. A Point in Time: Mutable Data in Online Inference

* speaker: Orr Shilon, Lemonade
* the data here are of the form "business point in time"
    - so this is like a "snapshot" of some attributes of a record, which change at different speeds
* "our users only want to time-travel to specific points in time"
    - new user
    - purchase
    - cancel

## Day 2

### A. The Only True Hard Problem in MLOps

* speaker: Todd Underwood, Google
* a lot of the problems described as MLOps problems are really just regular software problems (solved by SREs)
    - designing redundant infrastructure, treat configuration as code, distributed consensus
    - security controls, nonces, API-guarded access
    - monitoring/observability
    - fleet management - treat infrastructure as a fleet
* serving system SLOs
    - queries to models are fast and error-free
* operations and context-free decisions
    - each of these problems is solvable with context-free decisions (you don't need to know that the work happens to be machine learning)
    - those are the tractable problems in operations
* the ML in MLOps is mostly hidden in the data
    - distribution of the data is what matters most
    - subtle changes in the amount of data from various sources causes meaningful changes in the model
    - can't make context-free decisions
    - have to have the whole picture which is expensive and hard
* regular monitoring metrics like the volume of data isn't really enough to monitor a model
    - for example, if you are accidentally dropping all spanish-language data in a recommender that might not trigger any data-volume alerts, but all of a sudden your system will start making bad recommendations
* model quality can be impacted by both modeling failures AND infrastructure failures
    - training-serving skew
    - other feature definition changes
    - missing or corrupt or changed dependency
    - any bias in missing data
* KEY IDEA: model quality monitoring is the only real integration test of an ML system
* "I preferentially hire people who are good at distributed systems and interested in machine learning"
    - "you just don't need to know as much about machine learning as some people say, to do production machine learning"
* monitoring models: once you have a bit of confidence that your model is working well, you can kind of bootstrap model monitoring by checking "is the new version very very different from the previous version?"

### B. Towards a Unified Real-Time ML Data Pipeline, from Training to Serving

* speakers: Aakash Sabharwal, Sheila Hu (Etsy)
* "personalization and real-time signals are key to surfacing relevant content"
* Etsy is using a feature store that is populated by both batch features and "real-time" feature
* https://www.etsy.com/codeascraft

### C. Building a Best-in-Class Customer Experience Platform - The Hux Journey

* speakers:
    - Diego Oppenheimer (CEO of Algorithmia)
    - Paul Phillips (CTO, Hux CX Platform, Deloitte Digital)
    - Julian Berman (VP of Engineering, Hux CX Platform, Deloitte Digital)
* you're only as good as the weakest link in the architecture

### D. ML Design Patterns for Data Engineers

* speaker: Lak Lakshmanan (Google Cloud)
* O'Reilly book: "Machine Learning Design Patterns"

### E. Panel: Challenges of Operationalizing ML

* speakers
    - Alex Williams, Founder and Editor in Chief, The New Stack
    - Aparna Dhinakaran, CPO, Arize AI
    - Barr Moses, CEO & Co-Founder, Monte Carlo
    - Kevin Stumpf, Co-Founder and CTO, Tecton
* TL;DR the thing that makes MLOps "different" is about data distribution

### F. Financially Responsible Feature Engineering

* speakers: Joshua Hansen, Geoff Sims (Atlassian)
* Atlassian is using Tecton as their feature store
* they calculated the cost of every feature used by models
* basically the use of a feature stored allowed them to get a rich set of metrics for things like "how often was this feature updated", which gave them an idea about cost
    - they could also force data scientists to first consider using EXISTING features from other models
    - that's the whole value of a feature store in the first place!
* they saved $400K/year without sacrificing much accuracy just by dropping expensive features
    - one really interesting insight was that features which are specific to only a small set of records are really expensive because they aren't providing a lot of lift to the model but cost just as much to compute

### G. Reusability in Machine Learning

* speakers: Nayur Khan
* something something about Kedro
* github.com/quantumblacklabs/kedro

### H. Exploiting the Data/Code Duality with Dali

* speakers: Carl Steinbach (ex-LinkedIn)
* Conway's law = the systems built at organizations will mirror the structure of the organization
* "techniques permitting more efficient communication among designers will play an extremely important role in the design of technology systems"
* unless you're willing to maintain two copies of a data source, then you can't version it the way people version services
* Dali "gives users the ability to move seamlessly between physical and virtual datasets"

### I. miscellaneous discussion

* parallelM first invented the term "MLOps"
* DataRobot owns the copyright on the term "MLOps"

### J. The SAME Project

* speakers: David Aronchick (Microsoft)

### K. Something about FEAST

* you can very soon run FEAST on AWS and it will handle provisioning all of the relevant stuff

### L. Scaling a Machine Learning Social Feed with Feature Pipelines

* speakers:
    - Ettie Eyre, Platform Engineering Lead, Cookpad
    - Nadine Sarraf, Machine Learning Engineer, Cookpad
* were using Kuroka2 (a workflow engine)
* use case was personalizing recipe recommendations for users
* this company is using Argo for workflow management

### M. Programmatic Supervision for Software 2.0

* speakers:
    - Alex Ratner, Snorkel AI
* "ML development has a new bottleneck" --> the training data
* Snorkel used some theory from academia to come up with suggestiosn for how to generate training data

### N. Building a Feature Store to reduce the time to production of ML models

* speakers: Emiliano Martin (Mercado Libre)

### O. Feature Stores at Spotify: Building & Scaling a Centralized Platform

* speakers: Aman Khan (Spotify)
* features at Spotify
    - dynamic features
    - near "real time" features
    - complexity between trying to bridge offline and online
    - bridging online and offline features is tough
* as of 1 year ago, Spotify "didn't have a central strategy on features"
    - data scientists dealt with "data endpoints"
* `Jukebox`
    - Python and JVM components that help manage data
* think about scale from the very beginning

## Things to Google

* Argo
* BigTable
* `commuter` for docs (like sphinx)
* Direct3D
* Feast (open-source feature store)
* feature stores
* fragment shader
* GPGPU (general purpose GPU)
* Kedro
* Kuroko2 (or maybe Kuroka2?)
* metaflow
* multi-armed bandits
* nonces
* OpenGL 1.0
* Ray "actor"
* Samza
* Styx
* tecton
* `testbook` (notebook testing, "a unit testing framework for testing code in Jupyter notebooks")
* vertex shader
